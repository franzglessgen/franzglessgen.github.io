{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Knowledge base for the Hbb analyses in the ETH group</p>"},{"location":"VHbb/Home/","title":"VHbb analyses","text":"<p>The group is working on different analysis in the VH(bb) topology on Run2 and Run3 datasets. Here is a non-exhaustive contact list for the different sub-analyses.</p> <ul> <li>Run2 STXS analysis:  Alessandro Calandri, Krunal Gedia, Christina Reissel</li> <li>Run2 EFT analysis: Alessandro Calandri, Suman Chatterjee, Franz Glessgen, Vasilije Perovic</li> <li>Run3 analysis: in the future</li> </ul>"},{"location":"VHbb/UL/EFT/Intro/","title":"Introduction","text":"<p>This is an introduction</p>"},{"location":"VHbb/UL/EFT/fit/","title":"Fit model","text":""},{"location":"VHbb/UL/EFT/fit/#setup","title":"Setup","text":"<p>Follow these instructions:</p> <pre><code>export SCRAM_ARCH=slc7_amd64_gcc900\nscram project CMSSW CMSSW_11_3_4\ncd CMSSW_11_3_4/src\ncmsenv\ngit clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit\ncd HiggsAnalysis/CombinedLimit\ngit checkout v9.1.0\nscramv1 b clean; scramv1 b \ngit clone https://github.com/cms-analysis/CombineHarvester.git CombineHarvester\ncd CombineHarvester\ngit checkout v2.0.0\ngit clone https://github.com/franzglessgen/VHLegacyEFT.git VHLegacy\ncd VHLegacy\ngit checkout CH_v2\ncp model/EFTscaling*.py ../../HiggsAnalysis/CombinedLimit/python/\ncd ../..\nscram b \n</code></pre> <p>In order to implement the Wj1b ratio SF, the following files need to be copied to the Combine Harvester repository:</p> <pre><code>/work/fglessge/Combine/CMSSW_11_3_4/src/CombineHarvester/CombineTools/interface/CombineHarvester.h\n/work/fglessge/Combine/CMSSW_11_3_4/src/CombineHarvester/CombineTools/src/CombineHarvester_Creation.cc\n/work/fglessge/Combine/CMSSW_11_3_4/src/CombineHarvester/CombineTools/src/CombineHarvester_Datacards.cc\n</code></pre> <p>And that repo then needs to be recompiled.</p>"},{"location":"VHbb/UL/EFT/fit/#workflow","title":"Workflow","text":"<p>Shape location: /work/fglessge/CMSSW_10_1_0/src/CombineHarvester/VHLegacy/shapes/UL/BIT_resolved_boosted_NLO_Dphi/</p>"},{"location":"VHbb/UL/EFT/fit/#asimov-sensitivity","title":"Asimov sensitivity:","text":"<p>Workspace</p> <pre><code>python RunAll.py -comb_fit -years 2018,2017,2016,2016preVFP -outputdir BIT_resolved_boosted -iteration 1 -WC c0,c1,c2,c3,c6,c7\n</code></pre> <p>Scans</p> <pre><code>python RunAll.py -comb_fit -years 2018,2017,2016,2016preVFP -outputdir BIT_resolved_boosted -iteration 2 -WC c0,c1,c2,c3,c6,c7\n</code></pre> <p>Hadd</p> <pre><code>python RunAll.py -comb_fit -years 2018,2017,2016,2016preVFP -outputdir BIT_resolved_boosted -iteration 3 -WC c0,c1,c2,c3,c6,c7\n</code></pre> <p>Plot</p> <pre><code>python RunAll.py -comb_fit -years 2018,2017,2016,2016preVFP -outputdir BIT_resolved_boosted -iteration 4 -WC c0,c1,c2,c3,c6,c7\n</code></pre> <p>Split syst and stat : </p> <p>Do the first 3 iterations. Then redo iteration 2 and 3 by adding the command line argument \"-breakdown 1\", this creates the scans with frozen syst and hadds them. Finally, do iteration 4 with the same argument for plotting everything in the same graph.</p>"},{"location":"VHbb/UL/EFT/fit/#goodness-of-fit","title":"Goodness of fit:","text":"<pre><code>python RunAll.py -gof -years 2018 -outputdir BIT_resolved_boosted_mergeWjZjSF -iteration 1 -WC c6,c7 -skip_dc 1\n</code></pre> <p>And then iterations 2,3 and 4</p>"},{"location":"VHbb/UL/EFT/fit/#impact-plots","title":"Impact plots:","text":"<pre><code>python RunAll.py -impact -years 2018,2017,2016,2016preVFP -outputdir BIT_RB_split_udsgc_emu_TT_channel -iteration 1 -WC c0,c1,c2,c3,c6,c7\n</code></pre> <p>add skip_dc 0 if you want to recreate the WS. Can be ran on the WS of an asimov sensitivity scan.</p> <p>And then iterations 2,3 and 4</p>"},{"location":"VHbb/UL/EFT/fit/#pre-and-postfit","title":"Pre and postfit:","text":"<p>Workspace with reduced set of WC to save time</p> <pre><code>python RunAll.py -comb_prefit -years 2018,2017,2016preVFP,2016 -outputdir BIT_resolved_boosted_prefit -iteration 1 -WC c6,c7\n</code></pre> <p>Prefit : lasts around 10 to 20 hours</p> <pre><code>python RunAll.py -comb_prefit -years 2018,2017,2016preVFP,2016 -outputdir BIT_resolved_boosted_prefit -iteration 2 -WC c6,c7\n</code></pre> <p>Plot prefits: its quick</p> <pre><code>python RunAll.py -comb_prefit -years 2018,2017,2016preVFP,2016 -outputdir BIT_resolved_boosted_prefit -iteration 3 -WC c6,c7\n</code></pre> <p>Do full fit: 10 hours or so</p> <pre><code>python RunAll.py -comb_prefit -years 2018,2017,2016preVFP,2016 -outputdir BIT_resolved_boosted_prefit -iteration 4 -WC c6,c7\n</code></pre> <p>Do postfit : 10 to 20 hours</p> <pre><code>python RunAll.py -comb_prefit -years 2018,2017,2016preVFP,2016 -outputdir BIT_resolved_boosted_prefit -iteration 5 -WC c6,c7\n</code></pre> <p>Plotfit plots: quick</p> <pre><code>python RunAll.py -comb_prefit -years 2018,2017,2016preVFP,2016 -outputdir BIT_resolved_boosted_prefit -iteration 6 -WC c6,c7\n</code></pre>"},{"location":"VHbb/UL/EFT/fit/#combining-years","title":"Combining years:","text":""},{"location":"VHbb/UL/EFT/fit/#asimov-scans","title":"Asimov scans:","text":"<p>Same structure as before: the individual DC for each year have to be in the directories with the same name (BIT_resolved_boosted)</p> <pre><code>python RunAll.py -full_fit -years 2018,2017,2016preVFP,2016 -outputdir BIT_resolved_boosted -iteration 1 -WC c0,c1,c2,c3,c6,c7\n</code></pre> <p>and iteration 2/3/4</p>"},{"location":"VHbb/UL/EFT/fit/#2d-asimov-sensitivity","title":"2D Asimov sensitivity:","text":"<p>Run the comb_prefit_2D command, same as 1D. Start from the iteration 2 if you already produced the Asimov WS. This launches 8k jobs per WC pair and per year. </p> <p>For the full fit: first create the WS and DC and then</p> <pre><code>python RunAll.py -comb_fit_2D -years 2018 -outputdir BTV_BIT_resolved_boosted -iteration 2 -WC c0,c1,c2,c3,c6,c7 -full_fit_2D\n</code></pre>"},{"location":"VHbb/UL/EFT/lgbm/","title":"vhbbLightGBM","text":"<p>Workflow for training, evaluation and optimization of the Boosted Information Tree (BIT) as described here and there (ref)</p>"},{"location":"VHbb/UL/EFT/lgbm/#setup","title":"Setup","text":"<p>LightGBM can only be run in a python 3 environment. </p> <p>To source the conda environment on tier3, execute the following command:</p> <pre><code>PATH=/work/${USER}/miniconda3/bin:${PATH}\n</code></pre> <p>For details: </p> <pre><code>https://wiki.chipp.ch/twiki/bin/view/CmsTier3/HowToSetupYourAccount#local_Anaconda_Conda_installatio\n</code></pre>"},{"location":"VHbb/UL/EFT/lgbm/#root-compatibility","title":"ROOT compatibility","text":"<p>The training and optimization of the BIT does not require ROOT. However, for the eval step in Xbb the ROOT and python versions have to be compatible. Such a match between versions can be obtained by forging ROOT in a conda environment:</p> <p>Reference: https://iscinumpy.gitlab.io/post/root-conda/</p> <pre><code>conda create -n my_root_env root -c conda-forge\n</code></pre> <pre><code>conda activate my_root_env\n</code></pre> <pre><code>conda config --env --add channels conda-forge\n</code></pre> <p>You might need to add the extra packages like matplotlib, pandas, ... in the environment.</p> <p>This environment has a built-in pyroot3 version. </p> <p>Since you do not want to source the global tier3 CMSSW environment (it would overide your conda env), you need to run</p> <pre><code>export CMSSW_BASE={your_path}/CMSSW/\n</code></pre> <p>for setting the paths.</p> <p>Everytime you start a tier3 session, the following steps need to be taken:</p> <p>To get out of the default tier3 conda env</p> <pre><code>conda deactivate\n</code></pre> <pre><code>conda activate\n</code></pre> <p>Enter the pyroot3/py3 env</p> <pre><code>conda activate my_root_env\n</code></pre>"},{"location":"VHbb/UL/EFT/lgbm/#training","title":"Training","text":""},{"location":"VHbb/UL/EFT/lgbm/#export","title":"Export","text":"<p>Choose the features you need for the training in training.ini and make sure that the List_for_submitscript variable is set to the training region you want (resolved or boosted).</p> <pre><code>./submit.py -T Zll2018 -F cachetraining-BIT -J cachetraining --queue veryshort.q\n</code></pre> <p>Export the data as an h5 file</p> <pre><code>./submit.py -T Zll2018 -J export_h5 -i\n</code></pre>"},{"location":"VHbb/UL/EFT/lgbm/#setup_1","title":"Setup","text":"<p>Clone the repository and cd into it</p> <pre><code>git clone https://github.com/franzglessgen/vhbbLightGBM.git\n</code></pre> <p>Check your conda env by executing the training and saving it</p> <pre><code>python train.py -i ../dumps/DUMPNAME.h5 -name BITNAME_ -WC 1 -quad -train\n</code></pre> <p>This last command executes the training for a specified EFT component. The command line argument -WC defines the EFT index that is to be trained. The indexing is the same as in the reweighting card:</p> <pre><code>[0:'cHj1', \n 1:'cHj3', \n 2:'cHu', \n 3:'cHd', \n 4:'cHudRe', \n 5:'cHudIm', \n 6:'cHW', \n 7:'cHWtil', \n 8:'cHB', \n 9:'cHBtil', \n 10:'cHWB', \n 11:'cHWBtil', \n 12:'cHbox', \n 13:'cHDD', \n 14:'cbHRe', \n 15:'cbHIm']\n</code></pre> <p>Examples: -WC 1 -quad trains the quadratic component of cHj3. -WC 1,2 -quad trains the interaction term between cHj1 and cHu. -WC 1 (no -quad) trains the linear component of cHj3.</p> <p>The training also generates the result directories:</p> <ul> <li>trainings : saves the training arrays for this EFT component</li> <li>checkpoints : saves the BIT model for this EFT component</li> <li>plots</li> </ul>"},{"location":"VHbb/UL/EFT/lgbm/#merging","title":"Merging","text":"<p>In order to increase the statistical power of the training, the 4 years will be merged for training in each channel. A dedicated module dataMerger is used for the merging and to check that the distributions of features match across years. The command for merging 4 datasets that correspond to the exports of each year is the following:</p> <pre><code>python train.py -merger_input ../dumps/BIT_SRlowmedhigh_fullSet_Zll -merger_name BIT_SRlowmedhigh_fullSet_Zll -merge\n</code></pre> <p>For the command to work, the h5 files have to be named PREFIXYEAR, in this case:</p> <pre><code>BIT_SRlowmedhigh_fullSet_Zll2016.h5\n</code></pre> <p>for the 2016 dataset and similarly for the other years. The command loads the features for each year separately. The features as well as the weights can then be compared across years. </p>"},{"location":"VHbb/UL/EFT/lgbm/#training-and-plotting","title":"Training and plotting","text":"<p>The previous command loads the data from the h5 file, cleans the feature names and selects the features that are listed in the dictionary \"feature_dict\" in feature_config.py or feature_config_boost.py. Only the features listed in this dictionary with a key matching exactly the cleaned up name of the imported features will be considered. The dictionary also renames the features for more clarity.  The muons and electron features are then merged in a lepton category.</p> <p>The training parameters can be changed in the command line </p> <ul> <li>num_leaves</li> <li>max_depth</li> <li>min_data_in_leaf</li> <li>learning_rate</li> <li>max_bin (Light GBM binning of the features)</li> </ul> <p>If the -train argument is absent from the command line, the training will be loaded from a checkpoint (defined by the name used in the training with the -name argument).</p> <p>A few functions should be called to check the quality of the training:</p> <ul> <li>plot_features</li> <li>plot_weights: plots EFT weights used in training</li> <li>plot_score: plots output distribution for SIG and BG</li> <li>plot_feature_importance</li> <li>plot_all_profiled_features : plots the output score, profiled in the features to check the agreement between detector level quantities and joint quantities.</li> <li>plot_metrics : plots the loss function </li> </ul> <p>Additional function are available like plotting the BIT parameters or the correlation matrix of features, ...</p>"},{"location":"VHbb/UL/EFT/lgbm/#training-all-eft-components","title":"Training all EFT components","text":"<p>A simple script can be executed to perform the training for all EFT components. </p> <p>The command is </p> <pre><code>python RunTrainings.py -prefix Zll2018_BIT_ -WC 0,1,6,7 -train\n</code></pre> <p>The naming scheme of the h5 files (the Cuts) are hard-coded for boosted and resolved in the script RunTrainings.py.</p> <p>In this case the cuts are:</p> <pre><code>Cuts = [\"SRlowmedhigh_noDNNcut_allfeatures\", \"boosted_noDNNcut_allfeatures\"]\n</code></pre> <p>So the dump file is named:</p> <pre><code>prefix + cut\nZll2018_BIT_SRlowmedhigh_noDNNcut_allfeatures\n</code></pre> <p>This command then performs \\(2N + \\frac{N(N-1)}{2}\\) trainings, where N is the number of WC (defined by the indices).</p> <p>To obtain all plots related to the trainings do </p> <pre><code>python RunTrainings.py -prefix Zll2018_BIT_ -WC 0,1,6,7 -plot_training\n</code></pre> <p>The submission script actually runs the function gridsearch() which does a hyperparameter optimization on the fly and runs the training with the best parameters. The definition of the parameter grid is hard coded in \"grid search\". Because of the many trainings of this grid search, the resolved training might take a while (run on standard partition, should be done under 2 hours).</p> <p>Small issue with the training of the mixed component of chj1 and cHWtilde (c0 and c7).</p>"},{"location":"VHbb/UL/EFT/lgbm/#asimov-sensitivity-studies","title":"Asimov sensitivity studies","text":"<p>Once the individual trainings are done, they can be gathered for the Asimov studies.</p>"},{"location":"VHbb/UL/EFT/lgbm/#regressor-collection","title":"Regressor Collection","text":"<p>The Regressor collection loads BIT regressors of multiple WC at once. It can then be used to combine the shapes and perform LLH scans</p> <p>The syntax is similar to the training one. The WC indices can be specified. The regressor will then contain all EFT components corresponding to those. The name of the RegressorCollection has to match the name of the trainings to be loaded.</p> <p>For more flexibility, the BIT predictions and EFT weights of the different EFT components can be saved and simply loaded. LLH scans can be performed, the results of which can also be saved and loaded.</p>"},{"location":"VHbb/UL/EFT/lgbm/#loading","title":"Loading","text":"<p>Three functions can be called:</p> <ul> <li>read_arrays_from_training: combine the individual EFT components into a single file</li> <li>save_arrays: Drops the combined trainings into a h5 file</li> <li>read_arrays: Only function used after calling the 2 previous one. Only reads from the combined file. </li> </ul> <p>This command has to be called once before anything else.</p> <pre><code>python RunTrainings.py -prefix Zll2018_BIT_ -WC 0,1,6,7 -save_arrays\n</code></pre>"},{"location":"VHbb/UL/EFT/lgbm/#eft-scaling","title":"EFT scaling","text":"<p>Two different objects are needed for computing the optimal observables.</p>"},{"location":"VHbb/UL/EFT/lgbm/#eft-weight-scaling","title":"EFT weight scaling","text":"<p>The scaling of the EFT number of events (differential or inclusive) is controlled by the parameter \\(\\alpha\\) according to this formula:</p> <p>$$ N_{\\text{EFT}}(\\vec{\\alpha}) = S_0 + \\sum_i S_{1i} \\frac{\\alpha_i}{\\Lambda^2} +  \\sum_{j} S_{2j} \\frac{\\alpha_j^2}{\\Lambda^4} +  \\sum_{j,k} S_{3jk} \\frac{\\alpha_j \\alpha_k}{\\Lambda^4}  $$</p> <p>The function set_alphas stores the value of each EFT coefficient. The function set_scaling_alpha then computes the scaling for each EFT component according to the formula. After calling those, the function compute_EFT_weights stores the EFT scaling weights for all events at the considered WC point.</p>"},{"location":"VHbb/UL/EFT/lgbm/#likelihood-ratio-scaling","title":"Likelihood ratio scaling","text":"<p>The template shape also scales with the WC values according to the formula:</p> \\[R(x|\\theta, \\theta_0) = 1 + (\\theta - \\theta_0)_a R_a(x) + \\frac{1}{2}(\\theta - \\theta_0)_a(\\theta - \\theta_0)_bR_{ab}(x) \\] <p>where the indices \\(a\\) and \\(b\\) run over the WC. This second type of scaling is controlled by the parameter \\(\\theta\\)  and can be computed in a similar way to the \\(\\alpha\\) scaling.</p> <p>The \\(\\alpha\\) and \\(\\theta\\) scaling naming will be used throughout this documentation.</p>"},{"location":"VHbb/UL/EFT/lgbm/#workflow","title":"Workflow","text":"<p>The purpose of this analysis framework is to produce a 2D sigma matrix to choose the best 1D template shape for maximum sensitivity.</p>"},{"location":"VHbb/UL/EFT/lgbm/#checks","title":"Checks","text":"<p>You might need to produce a couple of preliminary plots to check the quality of the combined trainings.</p> <p>The script RunAll.py can be used to avoid lengthy command line arguments. It just needs the name of the trainings and the considered WC.</p>"},{"location":"VHbb/UL/EFT/lgbm/#features-and-score","title":"Features and score","text":"<p>The command:</p> <pre><code>python RunTrainings.py -prefix Zll2018_BIT_ -WC 0,1,6,7 -plot_template\n</code></pre> <p>will plot the features used in the training with the corresponding EFT effects. The EFT point is held in the dictionary WC_point_dict in scan_config. It will also plot the output shape at given \\(\\theta\\) and \\(\\alpha\\) points with a flat binning for the SM signal as well as EFT effects. </p>"},{"location":"VHbb/UL/EFT/lgbm/#optimal-1d-scan","title":"Optimal 1D scan","text":"<p>To check for the consistency of the training, running 1D optimal frozen scans for each WC can be done</p> <pre><code>python RunTrainings.py -prefix Zll2018_BIT_ -WC 0,1,6,7 -run_scans_1D\n</code></pre> <p>The range of the template points that will be scanned are defined in config/scan_config:</p> <ul> <li>theta_ranges : controls the range of the template shape points to try</li> <li>alpha_ranges : LLH range, be sure it holds the 2 sigma boundary on each WC.</li> </ul> <p>The command runs both the optimal 1D scan as well as one LLH scan for each choice of the template shape (number of \\(\\theta\\) points).</p> <p>The script CollectShapes.py also allows to run checks locally before/after submissions. Ex: - plot_sigma_variations : plots the 1D sigma scan</p>"},{"location":"VHbb/UL/EFT/lgbm/#2d-llh-scans","title":"2D LLH scans","text":"<p>The two following commands </p> <pre><code>python RunTrainings.py -prefix Zll2018_BIT_ -WC 0,1,6,7 -run_scans_2D_opt\n</code></pre> <pre><code>python RunTrainings.py -prefix Zll2018_BIT_ -WC 0,1,6,7 -run_scans_2D\n</code></pre> <p>will run all 2D scans needed for the selection of the template shape. For the optimal ones it will produce a 2D LLH scan for each pair of WC (the template shape is optimal at each point in the 2D LLH scan).</p> <p>For the non-optimal ones, it will go through the whole 2D range of \\(\\theta\\) points : \\(\\theta_i \\times \\theta_j\\) for each pair of WC.  For each grid point it will produce a 2D LLH scan by changing the template shape each time.</p> <p>This command launches a very large number of jobs:</p> \\[ \\frac{1}{2}N_{cat}N(N-1).Card[\\theta_i \\times \\theta_j] \\]"},{"location":"VHbb/UL/EFT/lgbm/#merge-scans","title":"Merge scans","text":"<p>Collect the results, fit the 1 and 2 sigma contours and produce a 2D sigma matrix for each WC pair:</p> <pre><code>python RunTrainings.py -prefix Zll2018_BIT_ -WC 0,1,6,7 -sigma\n</code></pre>"},{"location":"VHbb/UL/EFT/lgbm/#merge-matrices","title":"Merge matrices","text":"<p>Merge matrices and choose best template. This runs locally, should be short. Except for the minimization of the contraints on the \\(N\\times N\\) matrix of 2D matrices.</p> <pre><code>python RunTrainings.py -prefix Zll2018_BIT_ -WC 0,1,6,7 -merge_sigma\n</code></pre> <p>It also plots the chosen template shape.</p>"},{"location":"VHbb/UL/EFT/lgbm/#profiled-llh-scans","title":"Profiled LLH scans","text":"<p>Produce profiled LLH scans to check the robustness of the chosen template shape Do a stat only floating LLH scan for all WC: the 2 sigma intervals shouldn't blow up.</p> <pre><code>python RunTrainings.py -prefix Zll2018_BIT_ -WC 0,1,6,7 -profiler\n</code></pre>"},{"location":"VHbb/UL/EFT/lgbm/#evaluation","title":"Evaluation","text":""},{"location":"VHbb/UL/EFT/reweighting/","title":"Reweighting procedure","text":"<p>This Section provides a full description of all Xbb-specific computation and application of reweighting weights to MC samples to account for mismodellings and hardware deficiencies.</p>"},{"location":"VHbb/UL/EFT/reweighting/#list-of-reweighting-weights","title":"List of reweighting weights","text":"<p>The following weights are applied:</p> <ul> <li>Btag weights to account for the mismodelling in the lower than loose region of the btag score</li> <li>DeltaRbb weights to account for the MADGRAPH mismodelling at \\(\\Delta\\text{Rbb} &lt;1\\)</li> <li>\\(P_{t}(jj)\\) weights to account for the mismodelling observed at low \\(P_{t}(jj)\\) values</li> </ul>"},{"location":"VHbb/UL/EFT/reweighting/#impacted-cr","title":"Impacted CR","text":"<p>Here is a table summarizing the regions in which weights are applied. It is to be read from top to bottom, weights are computed for processes in pure regions before being applied in less pure regions to extract the weights for other processes there.</p> Channel Process Btag dRbb Ptjj DY LF LF + HF HF + LF Zll WJ \\(\\times\\) \\(\\times\\) \\(\\times\\) ZJ \\(\\times\\) \\(\\times\\) \\(\\times\\) ttbar \\(\\times\\) \\(\\times\\) \\(\\times\\) DY \\(\\times\\) LF + HF LF + HF Wlv WJ \\(\\times\\) LF +HF \\(\\times\\) ZJ \\(\\times\\) \\(\\times\\) \\(\\times\\) ttbar ttbar (if mis) \\(\\times\\) \\(\\times\\) DY \\(\\times\\) LF + HF \\(\\times\\) Zvv WJ \\(\\times\\) LF + HF \\(\\times\\) ZJ \\(\\times\\) LF + HF \\(\\times\\) ttbar \\(\\times\\) \\(\\times\\) \\(\\times\\)"},{"location":"VHbb/UL/EFT/reweighting/#2-lepton-treatment","title":"2-lepton treatment","text":"<p>In the 2-lepton channel, all three types of weights are applied. </p>"},{"location":"VHbb/UL/EFT/reweighting/#btag-weights","title":"Btag weights","text":"<p>The btag weights are applied in the ZLF only since it is the only region containing events satisfying btag &lt; loose. They are computed by taking the data/MC ratio in the 2D grid (btag1, btag0). The Xbb steps for computing them are as follows:</p> <ul> <li> <p>Run the plots for the following variable with a fine binning  <code>plotDef:DeepJet_2D_finebinning_2lep</code>  This is a finely binned 2D grid covering the lower than loose btag scores. The option : draw = RATIO has to be present as well as the option channelTag_for_reweight in Plot_general. A 2D plot with such options will enter the corresponding code in NewStackMaker. It will then remove all contributions that are not Z + light jets events from the data histogram and compute the ratio to MC Z + light.</p> </li> <li> <p>The previous plot is good for visualization, but the binning that was chosen for reweighting is the following: </p> </li> </ul> <pre><code>[-0.01, 0.01, 0.02333, LTL] \n</code></pre> <p>were LTL is the lower than loose boundary for the given year. The same binning is used for the low/med and high regions to have a good control on shape effects.</p> <ul> <li>The 2D plot with the previous binning has to be ran in 6 ZLF regions: Zee_low, Zee_med and Zee_high, Zmm_low, ...  </li> </ul> <pre><code>plotDef:DeepJet_2D_low_2lep\n</code></pre> <ul> <li>The logs will contain the central values and uncertainties of the btag weights.</li> <li>Copy those into the script: myutils/BTagWeightsDYltlooseWP (for 2lep) for each year and split in region</li> <li>Run the module Compute_btag_2D_2lep, it will fill branches with the btag weights (only for DY in Zll)</li> </ul> <pre><code>./submit.py -T Zll2018 -F weight_btag2D -J sysnew --addCollections Sys.Compute_btag_2D_2lep --input WEIGHTin --output WEIGHTout --queue veryshort.q\n</code></pre> <ul> <li>Apply the weights, rerun the caching/plotting to check closure in the btag weight 2D map and to check differences in other plots.</li> </ul>"},{"location":"VHbb/UL/EFT/reweighting/#deltarbb-weights","title":"deltaRbb weights","text":"<ul> <li> <p>Those weights have to be applied after the btag ones. The Xbb steps for computing them are as follows:</p> </li> <li> <p>Runplots on deltaRbb with FSR recovery in ZLF (inclusive in pT, split in ee and mm). The plot def is the following </p> </li> </ul> <pre><code>plotDef:deltaRbbWithFSR_UL_LF\n</code></pre> <ul> <li> <p>The binning is defined by 3 bins from 0.4 to 1.0.</p> </li> <li> <p>IMPORTANT: the overall normalization may be different between ZLF and ZHF. To compute it, the plot has an additional bin between 1.0 and 1.3. This bin has a large number of events and is in an orthogonal region to the low dijet pt region (see next section). The weights obtained in the &lt;1.0 bins have to be divided by the weight computed in the bin [1.0, 1.3]. This way, the overall normalization does not play a role. </p> </li> <li> <p>Get the central values and uncertainties for those 2 regions by adding the options draw = RATIO and draw_region = LF in the plot def</p> </li> <li>Copy the SF and uncertainties to the DeltaRbb module</li> </ul> <pre><code>NLORbblt1Weights_2lep.py\n</code></pre> <ul> <li>Run the module to apply those (in ZLF and ZHF)</li> </ul> <pre><code>./submit.py -T Zll2018 -F weight_deltaRbb -J sysnew --addCollections Sys.Compute_deltaRbb_2lep --input WEIGHTin --output WEIGHTout --queue veryshort.q\n</code></pre>"},{"location":"VHbb/UL/EFT/reweighting/#ptjj-weights","title":"Ptjj weights","text":"<p>After having applied the btag and deltaRbb weights, the pt(jj) has to be corrected at low values. The weights are computed in the ZLF region and then applied to ZLF and ZHF. The steps are as follows:</p> <ul> <li>Runplot for the variable in the ZLF region </li> </ul> <pre><code>Hpt_weights_low \n</code></pre> <p>with the binning:</p> <pre><code>[0,25.0,50.0,75.0,100.0,200.0]\n</code></pre> <p>The correction is differential in pt and in lepton flavour (6 regions). Do not forget the options draw = RATIO and draw_region = LF in the plot def.</p> <ul> <li> <p>Copy the central values and uncertainties in the module PtjjWeights.py</p> </li> <li> <p>Divide the central values by the overall normalization given by the last bin [100.0, 200.0]. IMPORTANT: only correct the dijet pt up to 100 GeV. The last bin is for computing this normalization so that it does not bias the application to HF.</p> </li> <li> <p>Run the module </p> </li> </ul> <pre><code>./submit.py -T Zll2018 -F Add_ptjj_weights -J sysnew --addCollections Sys.Compute_ptjj_2lep --input WEIGHT_pt_jj_in --output WEIGHT_pt_jj_out -i\n</code></pre>"},{"location":"VHbb/UL/EFT/reweighting/#1-lepton-treatment","title":"1-lepton treatment","text":"<p>The 1-lepton channel is more invested since both the DY and the WJ contributions need to be corrected.</p>"},{"location":"VHbb/UL/EFT/reweighting/#weights-for-dy-samples","title":"Weights for DY samples","text":"<p>The scale factors for DY samples have been computed in the regions of high Z+Jets purity. The same SF can thus be used in the Wlv channel. The btag weights were computed for both jets in the lower than loose region so this does not apply. Thus, only the ptjj and deltaRbb weights for DY samples are applied. Copy the weights from the previous modules into the PtjjAndDeltaRbbWeights module and use the following command:</p> <pre><code>./submit.py -T Wlv2018 -F Ptjj_deltaRbb_DY -J sysnew --addCollections Sys.Compute_ptjj_and_deltaRbb_DY_1lep --input DY_weights_in --output DY_weights_out --queue veryshort.q\n</code></pre> <p>You then need to recache the plots in the TT region for the next step</p>"},{"location":"VHbb/UL/EFT/reweighting/#btag-weights_1","title":"Btag weights","text":"<p>Btag weights still need to be applied in the WLF and TT CR because there is no btag score cut on the  subleading jet in these regions. Thus, the events in the lower than loose region need to be reweighted to account for the btag mismodelling there.  We chose to not apply the weights in the WLF region because the two bins to be corrected represent the largest part of the events. An overall normalization cannot be computed precisely because of this. Only apply the btag weight to TT</p> <p>To compute the SF, do the same in for the 2 lepton channel meaning:   - define a binning in btag score in the lower than loose region for the subleading jet  - add the options draw = RATIO and draw_region = TT   - run the plots in the TT region (split in e/m and pt med/high)  - copy the SF to the BTagWeights_WJ_TT_ltlooseWP script  - Divide them by the overall normalization  - run the script using the following command</p> <pre><code>./submit.py -T Wlv2018 -F btag_WJ_TT -J sysnew --addCollections Sys.Compute_btag_weights_WJ_TT_1lep --input Btag_weights_WJ_TT_in --output Btag_weights_WJ_TT_out --queue veryshort.q\n</code></pre> <ul> <li>For 2018, the ttbar was well modelled so these weights were skipped altogether. </li> <li>Fill the dictionary of WLF jets with 1.0 so that those weights are not used.</li> </ul> <p>You then need to recache the plots in the region WLF for the next step</p>"},{"location":"VHbb/UL/EFT/reweighting/#deltarbb-weights_1","title":"DeltaRbb weights","text":"<p>The deltaRbb weights are computed in WLF and applied to W+Jets in WLF and WHF. The steps are the same as for the btag weights, but the variable to be plotted is deltaRbb (split in 3 bins) in the region [0,1]. Do not forget to get the overall normalization in the region [1.0,5.0] and divide the central values and uncertainties by it.</p> <p>Copy the optained weights in NLORbblt1Weights_1lep (inclusive in pt and split in Wenu and Wmunu) and run the command to apply them:</p> <pre><code>./submit.py -T Wlv2018 -F deltaRbb_WJ -J sysnew --addCollections Sys.Compute_deltaRbb_weights_1lep --input DeltaRbb_weights_WJ_in --output DeltaRbb_weights_WJ_out --queue veryshort.q\n</code></pre> <p>You then need to recache the plots in the WLF for the next step</p>"},{"location":"VHbb/UL/EFT/reweighting/#0-lepton-treatment","title":"0-lepton treatment","text":"<p>For the 0-lep channel, simply apply the DeltaRbb weights. They are applied inclusively for all V+Jets samples (split in pt bins med/high).  There is an important difference in this channel because the overall MC normalization can be off. This overall normalization will be different in LF and HF so that the following steps need to be taken:</p> <ul> <li>Runplots on the deltaRbb variable with an additional bin from 1.0 to 4.0</li> <li>This last bin will give you the overall normalization of the LF</li> <li>The weights need to be divided by the overall normalization in LF but NOT in HF</li> <li>The relevant argument is self.overallNormalization_LF </li> </ul> <p>Then, run the corresponding module (NLORbblt1Weights_0lep) like that:</p> <pre><code>./submit.py -T Zvv2018 -F deltaRbb_VJets-v3 -J sysnew --addCollections Sys.Compute_deltaRbb_weights_0lep --input DeltaRbb_weights_in --output DeltaRbb_weights_out --queue veryshort.q\n</code></pre>"},{"location":"VHbb/UL/EFT/reweighting/#summary","title":"Summary","text":"<p>Here are all the steps for the full reweighting of a single year:</p>"},{"location":"VHbb/UL/EFT/systematics/","title":"Computation of systematics","text":""},{"location":"VHbb/UL/EFT/systematics/#2-lep-treatment","title":"2-lep treatment","text":"<p>Compute full set of MET syst. This replaces the syst branches MET_T1_pt/phi using the T1Smear uncertainties by propagating them through the METXY correction. </p> <pre><code>./submit.py -T Zll2018 -F Full_MET_syst -J sysnew --addCollections Sys.full_syst_MET --input Full_MET_syst_in --output Full_MET_syst_out -S ZHJet_HToBB_ZToLL_M-125_TuneCP5_SMEFTsim_topU3l_13TeV-madgraphMLM-pythia8 -i\n</code></pre> <p>Compute full set of syst for the Vboson (only in 0 and 1 lep since its influenced by MET)</p> <p>Computes full set of syst for corrected mSD:</p> <pre><code>./submit.py -T Zll2018 -F Full_mSD_syst -J sysnew --addCollections Sys.full_syst_mSD --input Full_msD_syst_in --output Full_msD_syst_out -S ZHJet_HToBB_ZToLL_M-125_TuneCP5_SMEFTsim_topU3l_13TeV-madgraphMLM-pythia8 -i\n</code></pre> <p>Then, compute the reduced set for Jet pt, mass Fatjet pt, mass and MET</p> <pre><code>./submit.py -T Zll2018 -F Reduced_syst -J sysnew --addCollections Sys.regroup_syst --input Reduced_syst_in --output Reduced_syst_out -S ZHJet_HToBB_ZToLL_M-125_TuneCP5_SMEFTsim_topU3l_13TeV-madgraphMLM-pythia8 --force -i\n</code></pre> <p>Then, recompute Higgs Candidate systematics. This will replace the erroneous syst with the correct, updated ones.</p> <pre><code>./submit.py -T Zll2018 -F Higgs_syst -J sysnew --addCollections Sys.HiggsReco --input Higgs_syst_in --output Higgs_syst_out -S ZHJet_HToBB_ZToLL_M-125_TuneCP5_SMEFTsim_topU3l_13TeV-madgraphMLM-pythia8 -i\n</code></pre> <p>Recompute kinfit stat: </p> <pre><code>./submit.py -T Zll2018 -F Kinfit_syst -J sysnew --addCollections KinematicFit.fitter --input Kinfit_syst_in --output Kinfit_syst_out -S ZHJet_HToBB_ZToLL_M-125_TuneCP5_SMEFTsim_topU3l_13TeV-madgraphMLM-pythia8 -i\n</code></pre> <p>Then, recompute EFT obs:</p> <pre><code>./submit.py -T Zll2018 -F EFT_obs_syst_2 -J sysnew --addCollections Sys.AddEFTobs --input EFT_obs_syst_2_in --output EFT_obs_syst_2_out -S ZHJet_HToBB_ZToLL_M-125_TuneCP5_SMEFTsim_topU3l_13TeV-madgraphMLM-pythia8 --force -i\n</code></pre> <p>Rerun is boosted for the systematics</p>"}]}